
# **EXP 5: Comparative Analysis of Different Prompting Strategies with Test Scenarios**

**Date:** 05.09.2025
**Register Number:** 212223060075

---

## **Aim**

To compare the effectiveness of naïve (broad/unstructured) prompts with refined (clear, structured) prompts in different real-world scenarios and evaluate responses based on **quality, accuracy, and depth**.

---

## **AI Tool Used**

* ChatGPT (OpenAI)

---

## **Definition of Prompt Types**

* **Naïve Prompt**: Simple, vague, or open-ended request without proper guidance.
* **Structured Prompt**: Detailed, contextual, and precise instruction that narrows down the expected response.

---

## **Methodology**

1. Identify multiple test scenarios: story generation, factual Q\&A, summarization, and advice.
2. Prepare two prompt versions for each task (naïve vs structured).
3. Run the experiment with ChatGPT and capture both outputs.
4. Evaluate outputs on:

   * **Quality** → Clarity & structure of the response
   * **Accuracy** → Correctness of information
   * **Depth** → Completeness & reasoning

---

## **Observations**

| **Scenario**   | **Naïve Prompt & Response**                                       | **Structured Prompt & Response**                                                                                                           | **Evaluation**                                   |
| -------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| Story Writing  | Prompt: “Write a story.” → Output: Very short, general story.     | Prompt: “Write a 150-word inspirational story about teamwork.” → Output: Focused, detailed, with a clear moral.                            | Structured prompt gave higher quality and depth. |
| Factual Answer | Prompt: “Explain about sun.” → Output: Few general facts.         | Prompt: “Explain the sun’s structure, energy source, and role in the solar system.” → Output: Detailed with scientific terms and accuracy. | Structured prompt improved accuracy.             |
| Summarization  | Prompt: “Summarize pollution.” → Output: Just 1–2 lines, unclear. | Prompt: “Summarize the causes, effects, and solutions of air pollution in under 120 words.” → Output: Organized, clear, and concise.       | Structured prompt gave clarity and depth.        |
| Advice         | Prompt: “Give me some tips.” → Output: Generic advice.            | Prompt: “Suggest three time-management tips for college students during exams.” → Output: Specific, actionable, and relevant.              | Structured prompt generated practical insights.  |

---

## **Analysis**

* Naïve prompts → produce vague, short, and unfocused responses.
* Structured prompts → deliver clear, detailed, and more accurate information.
* Across all scenarios, structured prompts consistently outperformed naïve prompts.
* Only for very **simple queries**, naïve prompts were acceptable.

---

## **Conclusion**

This experiment shows that **prompt clarity directly impacts AI output quality**. By providing structured instructions, ChatGPT responses become **richer, more accurate, and task-oriented**, making structured prompting the preferred approach for practical applications.

---

## **Result**

The comparative analysis of naïve and structured prompts was successfully carried out and the expected results were obtained.

---

